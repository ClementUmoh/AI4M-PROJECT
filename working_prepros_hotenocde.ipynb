{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Functions"
      ],
      "metadata": {
        "id": "Xbz2RoaXCq45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import split, explode, trim, regexp_replace, to_date, datediff, months_between, abs as Fabs, floor, col, lit\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 1. Preprocessing Functions\n",
        "# ==============================\n",
        "\n",
        "def preprocess_non_encoded(df):\n",
        "    \"\"\"\n",
        "    Preprocess dataset without one-hot encoding.\n",
        "    - Fill missing categorical values with the most frequent value.\n",
        "    - Fill missing numerical values with the median.\n",
        "    - If a column contains space-separated dates (e.g., \"01/01/2021 01/01/2022\"),\n",
        "      split them into separate rows.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Step 1: Handle missing values\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':  # Categorical columns\n",
        "            df[col].fillna(df[col].mode()[0], inplace=True)  # Fill with most frequent value\n",
        "        else:  # Numerical columns\n",
        "            df[col].fillna(df[col].median(), inplace=True)  # Fill with median\n",
        "\n",
        "    # Step 2: Check for space-separated date patterns and split into rows\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        if df[col].str.contains(r'\\d{2}/\\d{2}/\\d{4}').any():\n",
        "            df[col] = df[col].str.split()\n",
        "            df = df.explode(col).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def compute_months_diff(later, earlier):\n",
        "    \"\"\"Return full months difference between two datetimes (later - earlier).\"\"\"\n",
        "    if pd.isna(later) or pd.isna(earlier):\n",
        "        return pd.NA\n",
        "    years = later.year - earlier.year\n",
        "    months = later.month - earlier.month\n",
        "    total = years * 12 + months\n",
        "    # subtract one month if the day-of-month in `later` is earlier than `earlier`\n",
        "    if later.day < earlier.day:\n",
        "        total -= 1\n",
        "    return total\n",
        "\n",
        "def parse_stay(cell):\n",
        "    \"\"\"Return (days, location) based on rules:\n",
        "       - 'No' or 'No Answer' -> (0, same text)\n",
        "       - '<n> week(s)' -> days = n*7\n",
        "       - '<n> day(s)'  -> days = n\n",
        "       - If there is additional text besides the duration, treat that as location\n",
        "       - Pure duration only => location = pd.NA\n",
        "       - Unparsable => days = pd.NA\n",
        "    \"\"\"\n",
        "    if pd.isna(cell):\n",
        "        return pd.NA, pd.NA\n",
        "\n",
        "    s = str(cell).strip()\n",
        "    sl = s.lower().strip()\n",
        "\n",
        "    # treat common no-values\n",
        "    if sl in {\"no\", \"no answer\"}:\n",
        "        return 0, s  # days 0, location same string\n",
        "\n",
        "    # find a week or day duration anywhere\n",
        "    week_m = re.search(r'(\\d+)\\s*weeks?\\b', s, flags=re.IGNORECASE)\n",
        "    day_m  = re.search(r'(\\d+)\\s*days?\\b', s, flags=re.IGNORECASE)\n",
        "\n",
        "    days = pd.NA\n",
        "    matched_spans = []\n",
        "    if week_m:\n",
        "        weeks = int(week_m.group(1))\n",
        "        days = weeks * 7\n",
        "        matched_spans.append(week_m.span())\n",
        "    elif day_m:\n",
        "        days = int(day_m.group(1))\n",
        "        matched_spans.append(day_m.span())\n",
        "\n",
        "    # Remove any matched duration tokens to try to extract a location\n",
        "    loc_candidate = s\n",
        "    # Remove all duration patterns (e.g., \"2 weeks\", \"3 days\", \"2week\", \"1weeks\")\n",
        "    loc_candidate = re.sub(r'\\d+\\s*weeks?\\b', '', loc_candidate, flags=re.IGNORECASE)\n",
        "    loc_candidate = re.sub(r'\\d+\\s*days?\\b', '', loc_candidate, flags=re.IGNORECASE)\n",
        "    # remove common separators leftover\n",
        "    loc_candidate = re.sub(r'[,;:/\\-]+', ' ', loc_candidate)\n",
        "    loc_candidate = re.sub(r'\\s+', ' ', loc_candidate).strip()\n",
        "\n",
        "    # Decide location: show it when there is non-empty text besides a pure duration\n",
        "    if loc_candidate and loc_candidate.lower() not in {\"\", \"nan\"}:\n",
        "        location = loc_candidate\n",
        "    else:\n",
        "        # No extra text; for pure duration we want location = NaN\n",
        "        location = pd.NA\n",
        "\n",
        "    # Special rule: if the original was exactly '1 weeks' (or any pure duration) then location should be null\n",
        "    # already handled by loc_candidate being empty -> pd.NA\n",
        "\n",
        "    return (int(days) if pd.notna(days) else pd.NA), location\n",
        "\n",
        "def explode_and_diff(df, col1, col2):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Normalize and replace common separators with a single space\n",
        "    df[col2] = df[col2].astype(str).replace({r'[,;]': ' '}, regex=True)\n",
        "    df[col2] = df[col2].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "    df.loc[df[col2] == '', col2] = pd.NA\n",
        "\n",
        "    # Split into lists and explode into separate rows\n",
        "    df[col2 + \"_list\"] = df[col2].dropna().str.split(' ')\n",
        "    df_exp = df.explode(col2 + \"_list\").reset_index().rename(columns={'index': '_orig_index'})\n",
        "\n",
        "    # Parse dates (use dayfirst=True for dd/mm/yyyy)\n",
        "    df_exp[col1 + \"_dt\"] = pd.to_datetime(df_exp[col1], dayfirst=True, errors='coerce')\n",
        "    df_exp[col2 + \"_dt\"] = pd.to_datetime(df_exp[col2 + \"_list\"], dayfirst=True, errors='coerce')\n",
        "\n",
        "    # Signed differences\n",
        "    df_exp['days_diff_signed'] = (df_exp[col1 + \"_dt\"] - df_exp[col2 + \"_dt\"]).dt.days\n",
        "    df_exp['months_diff_signed'] = df_exp.apply(\n",
        "        lambda r: compute_months_diff(r[col1 + \"_dt\"], r[col2 + \"_dt\"]),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Convert negatives to positive while preserving NA\n",
        "    df_exp['Days_diff'] = df_exp['days_diff_signed'].apply(lambda x: abs(x) if pd.notna(x) else pd.NA)\n",
        "    df_exp['Months_diff'] = df_exp['months_diff_signed'].apply(lambda x: abs(x) if pd.notna(x) else pd.NA)\n",
        "\n",
        "    # If you don't want to keep signed columns, drop them:\n",
        "    df_exp = df_exp.drop(columns=['days_diff_signed', 'months_diff_signed'])\n",
        "    df_exp.fillna(0)\n",
        "\n",
        "    colname = \"How would you describe the severity of your malaria symptoms during your most recent diagnosis?\"\n",
        "    pattern = r'^(.*?)\\s*\\((.*?)\\)\\s*$'\n",
        "    s = df[colname].astype(str)\n",
        "    extracted = s.str.extract(pattern)\n",
        "\n",
        "    # create new columns\n",
        "    df_exp['Severity_Main'] = extracted[0].fillna(df_exp[colname]).str.strip()\n",
        "    df_exp['Severity_Type'] = extracted[1].replace({r'^\\s*$': None}, regex=True).str.strip()\n",
        "\n",
        "    df_exp['Severity_Type'] = df_exp['Severity_Type'].where(df_exp['Severity_Type'] != '', pd.NA)\n",
        "\n",
        "    # Apply parser and create new columns\n",
        "    parsed = df_exp['If Yes, specify the location(s) and duration of stay'].apply(parse_stay)\n",
        "    df_exp[['Stay_Numbr_Days', 'Stay_Location']] = pd.DataFrame(parsed.tolist(), index=df_exp.index)\n",
        "\n",
        "    # choose integer nullable dtype for days\n",
        "    df_exp['Stay_Numbr_Days'] = df_exp['Stay_Numbr_Days'].astype('Int64')\n",
        "\n",
        "    parsed = df_exp[col].apply(parse_months)\n",
        "    df_exp[['months_list', 'months_num_list', 'season_label']] = pd.DataFrame(parsed.tolist(), index=df.index)\n",
        "\n",
        "    df_exp.fillna(0)\n",
        "\n",
        "    return df_exp\n",
        "\n",
        "def preprocess_one_hot(df):\n",
        "    \"\"\"\n",
        "    Preprocess dataset and one-hot encode categorical columns.\n",
        "\n",
        "    Steps:\n",
        "    - Convert list/tuple/set entries in object columns to comma-separated strings\n",
        "    - Fill missing categorical values with the mode (most frequent value)\n",
        "    - Fill missing numeric values with the median\n",
        "    - Skip datetime columns from encoding\n",
        "    - One-hot encode categorical columns\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import inspect\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert lists/tuples/sets in object columns to strings\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = df[col].apply(lambda x: ','.join(map(str, x)) if isinstance(x, (list, tuple, set)) else x)\n",
        "\n",
        "    # Fill missing values\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_object_dtype(df[col]):\n",
        "            mode_vals = df[col].mode(dropna=True)\n",
        "            fill_val = mode_vals.iloc[0] if not mode_vals.empty else ''\n",
        "            df[col] = df[col].fillna(fill_val)\n",
        "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
        "            # Fill missing dates with earliest available date\n",
        "            min_date = df[col].min()\n",
        "            df[col] = df[col].fillna(min_date)\n",
        "\n",
        "    # Identify categorical columns (excluding datetime)\n",
        "    categorical_cols = [\n",
        "        col for col in df.select_dtypes(include=['object']).columns\n",
        "    ]\n",
        "\n",
        "    if not categorical_cols:\n",
        "        return df\n",
        "\n",
        "    # Configure OneHotEncoder for sklearn version compatibility\n",
        "    params = {'handle_unknown': 'ignore'}\n",
        "    sig = inspect.signature(OneHotEncoder)\n",
        "    if 'sparse_output' in sig.parameters:\n",
        "        params['sparse_output'] = False\n",
        "    else:\n",
        "        params['sparse'] = False\n",
        "\n",
        "    encoder = OneHotEncoder(**params)\n",
        "    encoded = encoder.fit_transform(df[categorical_cols])\n",
        "\n",
        "    if hasattr(encoded, \"toarray\"):\n",
        "        encoded = encoded.toarray()\n",
        "\n",
        "    encoded_df = pd.DataFrame(\n",
        "        encoded,\n",
        "        columns=encoder.get_feature_names_out(categorical_cols),\n",
        "        index=df.index\n",
        "    )\n",
        "\n",
        "    # Combine with non-categorical data\n",
        "    df_out = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
        "    return df_out\n"
      ],
      "metadata": {
        "id": "AzeS8zUpDtwE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = \"If Yes, specify months\"   # replace with your real column name\n",
        "\n",
        "# canonical month order and maps\n",
        "months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "          \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
        "name_to_num = {m.lower(): i+1 for i, m in enumerate(months)}\n",
        "abbr_to_full = {\n",
        "    'jan':'January','feb':'February','mar':'March','apr':'April','may':'May','jun':'June',\n",
        "    'jul':'July','aug':'August','sep':'September','sept':'September','oct':'October',\n",
        "    'nov':'November','dec':'December'\n",
        "}\n",
        "\n",
        "month_pattern = r'\\b(' + '|'.join([m.lower() for m in months] + list(abbr_to_full.keys())) + r')\\b'\n",
        "month_regex = re.compile(month_pattern, flags=re.IGNORECASE)\n",
        "\n",
        "def normalize_token(tok):\n",
        "    tok = tok.strip().strip(' ,;:-').lower()\n",
        "    if tok in abbr_to_full:\n",
        "        return abbr_to_full[tok]\n",
        "    # full month spelled out?\n",
        "    for m in months:\n",
        "        if tok == m.lower():\n",
        "            return m\n",
        "    return None\n",
        "\n",
        "def expand_range(start_name, end_name):\n",
        "    \"\"\"Return inclusive list of month names between start and end, wrapping year if needed.\"\"\"\n",
        "    s = name_to_num[start_name.lower()]\n",
        "    e = name_to_num[end_name.lower()]\n",
        "    if s <= e:\n",
        "        nums = list(range(s, e+1))\n",
        "    else:  # wrap around (e.g., Nov-Feb)\n",
        "        nums = list(range(s, 13)) + list(range(1, e+1))\n",
        "    return [months[n-1] for n in nums]\n",
        "\n",
        "def parse_months(cell):\n",
        "    if pd.isna(cell):\n",
        "        return pd.NA, pd.NA, pd.NA  # months_list, months_num_list, season_label\n",
        "\n",
        "    s = str(cell).strip()\n",
        "\n",
        "    # treat obvious unknown answers\n",
        "    if s.lower() in {\"i don't know\", \"dont know\", \"dont know.\", \"no\", \"no answer\", \"unknown\", \"i dont know\"}:\n",
        "        return pd.NA, pd.NA, s\n",
        "\n",
        "    # standardize separators and remove trailing commas\n",
        "    s_clean = re.sub(r'[,;/]+', ' ', s)\n",
        "    s_clean = re.sub(r'\\s+', ' ', s_clean).strip()\n",
        "\n",
        "    # 1) direct month names anywhere\n",
        "    found = month_regex.findall(s_clean)  # list of matches\n",
        "    found = [normalize_token(x) for x in found if normalize_token(x) is not None]\n",
        "\n",
        "    # If we have two month names and a hyphen in original, treat it as a range and expand\n",
        "    hyphen_range_match = re.search(r'([A-Za-z]+)\\s*[-â€“]\\s*([A-Za-z]+)', s_clean)\n",
        "    if hyphen_range_match:\n",
        "        a = normalize_token(hyphen_range_match.group(1))\n",
        "        b = normalize_token(hyphen_range_match.group(2))\n",
        "        if a and b:\n",
        "            expanded = expand_range(a, b)\n",
        "            nums = [name_to_num[m.lower()] for m in expanded]\n",
        "            return expanded, nums, pd.NA\n",
        "\n",
        "    # If multiple explicit months found (commas or spaces), return unique ordered list\n",
        "    if found:\n",
        "        # remove duplicates while preserving first-occurrence order\n",
        "        seen = []\n",
        "        for m in found:\n",
        "            if m not in seen:\n",
        "                seen.append(m)\n",
        "        nums = [name_to_num[m.lower()] for m in seen]\n",
        "        return seen, nums, pd.NA\n",
        "\n",
        "    # no explicit month names found: capture season/words like 'Harmattan', 'Raining', etc.\n",
        "    season_keywords = ['harmattan', 'raining', 'rainy', 'rain', 'dry season', 'wet season', 'monsoon']\n",
        "    lower = s.lower()\n",
        "    for kw in season_keywords:\n",
        "        if kw in lower:\n",
        "            return pd.NA, pd.NA, s  # put original text in season_label\n",
        "\n",
        "    # fallback: nothing useful found\n",
        "    return pd.NA, pd.NA, s  # keep original as season_label so you can inspect\n",
        "\n"
      ],
      "metadata": {
        "id": "ixMz74wPLkjP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot Encoding"
      ],
      "metadata": {
        "id": "NHDbiy6aCrj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 2. Load and Process Dataset\n",
        "# ==============================\n",
        "\n",
        "# Load dataset (Upload manually in Colab)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get file name (assuming CSV format)\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Read dataset\n",
        "df = pd.read_csv(file_name)"
      ],
      "metadata": {
        "id": "oaBcKHzetkRN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "6b5bdd1f-14dc-44a3-ec87-dc1d990cdd4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-880ae22b-0d0f-4aed-bdee-c6de4c61b722\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-880ae22b-0d0f-4aed-bdee-c6de4c61b722\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving AI4M Raw Data Set - Form Responses 1.csv to AI4M Raw Data Set - Form Responses 1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PreProcessing and Cleaning data"
      ],
      "metadata": {
        "id": "_9RdFPC9tocX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess without encoding\n",
        "col1 = \"Date of diagnosis\\nOn what date were you last diagnosed with malaria?\"\n",
        "col2 = \"If diagnosed multiple times, list the dates of diagnosis in the past year\"\n",
        "\n",
        "df_non_encoded = preprocess_non_encoded(df)\n",
        "sc = explode_and_diff(df_non_encoded,col1,col2)\n",
        "print(\"Preprocessed Data (Non-Encoded):\")\n",
        "display(sc)"
      ],
      "metadata": {
        "id": "Vci5DnMS_DN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "sc.to_csv('preproessed_output.csv', index=False)   # save to disk in Colab\n",
        "files.download('preproessed_output.csv')"
      ],
      "metadata": {
        "id": "u3FBuzkstm_a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2f5fee15-c7c9-493d-b97b-fcc05880b58b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9688d4de-a678-4739-ae64-e95e298ebe16\", \"preproessed_output.csv\", 61305)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Hot Encoding"
      ],
      "metadata": {
        "id": "V7Wbyc4ntv_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess with one-hot encoding\n",
        "df_one_hot = preprocess_one_hot(sc)\n",
        "print(\"Preprocessed Data (One-Hot Encoded):\")\n",
        "display(df_one_hot)"
      ],
      "metadata": {
        "id": "4mm6yua5EUOZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "df_one_hot.to_csv('hotencoded_output.csv', index=False)   # save to disk in Colab\n",
        "files.download('hotencoded_output.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "hUGMT3pTc0N4",
        "outputId": "983d2ea5-5a86-4ba8-e800-127067d60aed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a1948f5e-3b55-4319-8b23-635a2b5673b6\", \"hotencoded_output.csv\", 307610)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit Testing"
      ],
      "metadata": {
        "id": "PhCKG04SCr9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 3. Unit Testing\n",
        "# ==============================\n",
        "\n",
        "class TestPreprocessing(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit tests for preprocessing functions.\n",
        "    - Check missing value handling in non-encoded preprocessing.\n",
        "    - Validate one-hot encoding transformation.\n",
        "    \"\"\"\n",
        "    def setUp(self):\n",
        "        \"\"\"\n",
        "        Create a sample dataset with missing and categorical data.\n",
        "        \"\"\"\n",
        "        self.data = pd.DataFrame({\n",
        "            'Category': ['A', 'B', np.nan, 'A', 'C'],\n",
        "            'Value': [10, 20, 30, np.nan, 50]\n",
        "        })\n",
        "\n",
        "    def test_non_encoded_preprocessing(self):\n",
        "        \"\"\"\n",
        "        Test preprocess_non_encoded function.\n",
        "        - Ensure no missing values remain.\n",
        "        - Check categorical values are correctly filled.\n",
        "        \"\"\"\n",
        "        processed_df = preprocess_non_encoded(self.data)\n",
        "        self.assertFalse(processed_df.isnull().values.any(), \"There should be no missing values.\")\n",
        "        self.assertIn(processed_df['Category'][2], ['A', 'B', 'C'], \"Categorical missing values should be filled.\")\n",
        "\n",
        "    def test_one_hot_encoding(self):\n",
        "        \"\"\"\n",
        "        Test preprocess_one_hot function.\n",
        "        - Check if categorical columns are transformed correctly.\n",
        "        - Validate column naming after one-hot encoding.\n",
        "        \"\"\"\n",
        "        processed_df = preprocess_one_hot(self.data)\n",
        "        self.assertFalse(processed_df.isnull().values.any(), \"There should be no missing values.\")\n",
        "        self.assertTrue(all(col.startswith('Category_') for col in processed_df.columns if 'Category_' in col), \"One-hot encoding should create correct column names.\")\n",
        "\n",
        "# Run unit tests\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestPreprocessing))"
      ],
      "metadata": {
        "id": "_DC9CdJlEUrl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}